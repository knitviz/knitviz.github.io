<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-113771934-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-113771934-1');
    </script>
    <meta charset="UTF-8">
    <meta name="description" content="Malena Hansen's Online Portfolio Showcasing a User Research Project">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr"
        crossorigin="anonymous">
    <link rel="icon" type="image/png" href="m-favicon.png">
    <title>Malena Hansen - Survey Platform Research</title>
</head>

<body>
    <div class="container" id="project-pages">
        <div class="header">
            <a href="index.html">home</a> &colon; &colon; <a href="research.html">research projects</a>
            <h1>Research Project: Survey Platform</h1>
        </div>

        <div id="menu">
            <h3>Project Details</h3>
            <dl>
                <dt>feature focus</dt>
                <dd>Survey Platform</dd>

                <dt>completed</dt>
                <dd>Jan 2019</dd>

                <dt>my team</dt>
                <dd>Product Manager, Product Owner, EVP Product, Tech/Dev Lead, Developers, QA, UX</dd>

                <dt>my role</dt>
                <dd>UX: Evaluative User Researcher, Lead Designer</dd>

                <dt>tools used</dt>
                <dd>Pencil, Paper, Survey&nbsp;Monkey, Ring Central, Metabase, Excel, OptimalSort, Sketch&nbsp;App,
                    InVision&nbsp;Studio</dd>

                <dt>evaluated</dt>
                <dd>Product Ideas, User Need</dd>
            </dl>
            <hr>
            <h3>Project Segments</h3>
            <ul>
                <li class="active">research</li>
                <li><a href="ia-survey.html">information architecture</a></li>
                <li><a href="design-survey.html">website design</a></li>
            </ul>

        </div>

        <div id="content">
            <h2>problem</h2>
            <p>We know that our customers in the skilled nursing and senior living markets face high employee turnover,
                and had a theory that by evaluating their employees' engagement with a series of surveys, we'd better
                be able to help our customers act on information before it was too late to impact any changes in the
                organization that may have kept an employee happy&nbsp;/ engaged.</p>

            <h3>Previous Research</h3>
            <p>The generative research work for the new survey types had been completed by the time I joined the Engage
                team as a UX Designer.</p>

            <h3>My Tasks</h3>
            <p>My research work was evaluative and focused on running the proof of concept surveys before developers
                were brought on, to determine if the response rates for our regular pulse survey were negatively
                impacted by our increasing the frequency&nbsp;/ adding new types of surveys that employees were
                receiving.</p>
            <p>There were a number of things we were looking to evaluate with the survey platform proof of concept:
                <ol>
                    <li>adding a new hire survey program</li>
                    <li>creating a quaretly survey program</li>
                    <li>including a second required question with the weekly pulse survey</li>
                </ol>
            </p>
            <p>Our customers had told us that they wanted to be able to ask their own questions outside of the pulse
                survey, but we didn't have a good way of including that survey type in our development-light proof of
                concept, so it was something we could ask about during our evaluative research, but it would have to be
                something we evaluated in the product at a later time.</p>

            <h2>actions</h2>
            <p>The first thing that needed to be done was determine which of the items we were hoping to evaluate
                should be the first.</p>
            <p>To determine which should be first, my product manager and I had a conversation with the
                developer&nbsp;/ tech lead to learn what the level of effort would be for the things that we would need
                developer help for:
                <ul>
                    <li>the 2-question pulse proof of concept</li>
                    <li>the create your own survey pilot</li>
                </ul>
                My product manager then evaluated the product priorities and worked with me to create my design&nbsp;/
                experience priorities for the proof of concepts and survey pilots.
            </p>
            <p>And finally, we had to find a customer to partner with us so that we could run our proof of concept
                tests. The customer that we asked to partner with us for the proof of concept studies was selected
                because of their higher than average pulse survey response rate of <span class="h3">38.45%</span>
                instead of just <span class="h3">19.82%</span>.</p>

            <h3>My Priorities</h3>
            <p>I was tasked with the running of the developer-free proof of concepts for New Hire Surveys and Quarterly
                Surveys using existing OnShift Schedule messaging functionality with our surveys built out in Survey
                Monkey.</p>
            <p>Once we had secured a customer-partner for the proof of concept, I was tasked with running the proof of
                concepts: from manually sending the surveys to analyzing the data.</p>

            <h4>New Hire Surveys</h4>
            <p>The key part of the new hire surveys was that they were sent to an employee at specific tenure
                milestones, so a newly-hired employee would receive a survey after:
                <ul>
                    <li>their <span class="h3">7th</span> day of employment, </li>
                    <li>their <span class="h3">14th</span> day of employment,</li>
                    <li>their <span class="h3">30th</span> day of employment,</li>
                    <li>their <span class="h3">60th</span> day of employment,</li>
                    <li>and their <span class="h3">90th</span> day of employement</li>
                </ul>
                ...at which point they were no longer considered a "new employee."</p>
            <p>At the beginning and middle of every week during the proof of concept, I checked for new employees and
                added their hire date to my Excel sheet so I knew when to send each employee the correct survey. Once I
                knew when to send each survey to each newly hired employee, I set a calendar reminder to send the
                survey.</p>
            <p>Using the existing OnShift Schedule messaging platform, when it was time to send a new employee their
                surveys, I created a new message and sent the employee an intro message with a link to the survey.</p>
            <p>The new hire survey proof of concept ran for 4 months to account for the fluctuation in employee
                hiring&nbsp;+ onboarding and to give us the opportunity to follow the full 90-day protocol for new
                hires.</p>

            <h4>Quarterly Surveys</h4>
            <p>The process was a little different for the quarterly survey proof of concept: all employees who had been
                employed for over 90 days were sent a survey on a specific theme once per month.</p>
            <p>It was my responsibility to verify that an individual was still employed and that any recently hired
                employees who had made it to day 91 were added to the group of employees who received the monthly
                survey. And then once a month I followed the same procedure as I used for the new hire surveys to send
                all the employees with at least a 91-day tenure a survey on <span class="h3">Communication</span>
                (month 1), <span class="h3">Supervisor Support</span> (month 2), or <span class="h3">Cooperation</span>
                (month 3).</p>
            <p>The quarterly survey proof of concept ran for the same 4 months as the new hire proof of concept so that
                we could evaluate the impact of recieving a longer survey each month quarter over quarter, the impact
                the longer survey had
                on the pulse survey results, and determine if the new hire survey results impacted later quarterly
                survey results from the same individuals.</p>

            <h4>2-Question Pulse A/B Test</h4>
            <p>With the help of a Senior UX Designer, I was able to convince management that going forward with
                development for the customer-heard request of adding a second to the pulse survey might not be the best
                idea - and we should pause to do some evaluation by way of an A/B test to compare the response +
                completion rates for the different survey types.</p>
            <p>This would require development time and effort, so it wasn't something that we could start right away,
                but I knew that the effort would be worth it.</p>
            <p>Looking back in our database, I was able to sample the employees and evaluate who regularly responded to
                pulse surveys, how often they left comments, and how often they chose to remain anonymous when
                responding to a pulse survey. Using an Excel Pivot Table, I used the information from the database to
                split out partner's employees into 2 equal groups of responders and non-responders to ensure an even
                split between which group would continue to receive the regular pulse survey and the group that would
                receive the 2-question pulse survey.</p>

            <h2>results</h2>
            <p>To reduce the number of variables in our A/B 2-question pulse survey test and to allow the development
                staff a chance to plan for our A/B test effort, we broke out the proof of concepts to separate time
                frames: the new hire + quarterly survey programs (from May 2018 through August 2018) and the 2-question
                pulse survey (from mid-November 2018 through early January 2019).</p>

            <h3>New Survey Programs</h3>
            <p>At the end of every week, I downloaded the new hire survey results from Survey Monkey and created an
                Excel report that displayed the information in a chart for our customer-partner. The Monday of the
                following week, we would review the survey comments and response rate during a weekly phone call set-up
                for the purposes of reviewing the proof of concept data.</p>
            <p>While the new survey programs consist of different development epics, the proof of concept for each
                program was run at the same time to get information about the success&nbsp;/ failure of each program
                faster and because the groups that would be participating in each survey program was mutually
                exclusive.</p>
            <p>During my data analysis, I saw that the pulse survey response rate declined during the new survey
                platform proof of concept and I believe that it was due to the increased number of surveys that more
                tenured employees were recieving: they were being asked to complete an additional survey of 4 - 6
                questions once a month on top of their regularly scheduled weekly pulse surveys vs the newly hired
                employees who weren't used to any number of surveys and who had a lower response rate over all.</p>

            <h4>New Hire Surveys</h4>
            <p>The response rate of the new hire surveys was lower than expected at <span class="h3">27.95%</span>
                based on the raw response rate numbers from the pulse survey; however, the lower rate may be explained
                by a lower awareness of the existing pulse survey program by newly hired employees.</p>
            <table>
                <caption>Lifetime new hire response rate.</caption>
                <thead>
                    <tr>
                        <th scope="col">Survey Type</th>
                        <th scope="col">Response Rate</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="info">Day 7</td>
                        <td class="info">11.76%</td>
                    </tr>
                    <tr>
                        <td class="info">Day 14</td>
                        <td class="info">26.32%</td>
                    </tr>
                    <tr>
                        <td class="info">Day 30</td>
                        <td class="info">33.33%</td>
                    </tr>
                    <tr>
                        <td class="info">Day 60</td>
                        <td class="info">35.00%</td>
                    </tr>
                    <tr>
                        <td class="info">Day 90</td>
                        <td class="info">33.33%</td>
                    </tr>
                </tbody>
            </table>

            <h4>Quarterly Surveys</h4>
            <p>The quarterly survey response rate started about the same as the pulse survey response rate; however,
                over the time of the proof of concept, the response rate for the quarterly survey itself declined by
                <span class="h3">5.42%</span> and the weekly pulse survey rate declined by <span class="h3">3.96%</span>
                as well during the 4 month period.</p>

            <table>
                <caption>Pulse survey response rates vs quarterly survey response rates.</caption>
                <thead>
                    <tr>
                        <th scope="col" rowspan="2">Month</th>
                        <th scope="col" colspan="2">Survey Type</th>
                    </tr>
                    <tr>
                        <th scope="col">Pulse Survey</th>
                        <th scope="col">Quarterly Survey</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td colspan="3" class="sub-head">Pre-Proof of Concept Pulse Results</td>
                    </tr>
                    <tr>
                        <td class="info">February 2018</td>
                        <td class="info">44.49%</td>
                        <td class="info">n/a</td>
                    </tr>
                    <tr>
                        <td class="info">March 2018</td>
                        <td class="info">43.00%</td>
                        <td class="info">n/a</td>
                    </tr>
                    <tr>
                        <td class="info">April 2018</td>
                        <td class="info">43.28%</td>
                        <td class="info">n/a</td>
                    </tr>
                    <tr>
                        <td colspan="3" class="sub-head">Pulse + Proof of Concept Results</td>
                    </tr>
                    <tr>
                        <td class="info">May 2018</td>
                        <td class="info">49.23%</td>
                        <td class="info">42.86% <br>(communication)</td>
                    </tr>
                    <tr>
                        <td class="info">June 2018</td>
                        <td class="info">46.82%</td>
                        <td class="info">39.50% <br>(supervisor support)</td>
                    </tr>
                    <tr>
                        <td class="info">July 2018</td>
                        <td class="info">46.32%</td>
                        <td class="info">40.00% <br>(cooperation)</td>
                    </tr>
                    <tr>
                        <td class="info">August 2018</td>
                        <td class="info">45.27%</td>
                        <td class="info">37.44% <br>(communication)</td>
                    </tr>
                    <tr>
                        <td colspan="3" class="sub-head">Post-Proof of Concept Pulse Results</td>
                    </tr>
                    <tr>
                        <td class="info">September 2018</td>
                        <td class="info">50.09%</td>
                        <td class="info">n/a</td>
                    </tr>
                    <tr>
                        <td class="info">October 2018</td>
                        <td class="info">51.90%</td>
                        <td class="info">n/a</td>
                    </tr>
                    <tr>
                        <td class="info">November 2018</td>
                        <td class="info">54.24%</td>
                        <td class="info">n/a</td>
                    </tr>
                </tbody>
            </table>


            <h3>2-Question Pulse Survey</h3>
            <p>There were 3 pieces of information that I was concered about during our A/B test:
                <ul>
                    <li>survey completion rate: how many people started and then finished the survey</li>
                    <li>survey response rate: who started to take the pulse survey variant</li>
                    <li>if there was a difference in either rate between our test variants</li>
                </ul>
            </p>
            <p>We had established a preferred and less-preferred variant for our "B" option in the A/B test; due to
                development time, we were only able to test our less-preferred variant - and I believe that if we had
                been able to test our more-preferred variant, the results might be slightly different.</p>
            <p>What I saw when I looked at the results was very interesting: overall (across all customers responding
                to the pulse survey) the results were very consistent and for our partner, as the A/B test continued,
                while there was still some week to week variance in completion rates, as the test continued, the
                responses for our "B" variant climbed to almost meet the default "A" variant. I suspect that there may
                be a couple of things at work here:
                <ol>
                    <li>there was no initial communication to the employees who were in the "B" group and because there
                        was no signifier that their regular pulse survey had changed until the normal end of the
                        survey, the employees didn't initially percieve that there was any difference and left the page
                        when they would normally be done</li>
                    <li>as the A/B test continued, if those employees who hadn't been completing the surveys noticed
                        that they hadn't been getting their normal survey completion points, they may have paid more
                        attention as they completed - or thought they completed - their next set of surveys</li>
                </ol>
            </p>

            <table>
                <caption>Completion rate by week for all customer pulse survey responses, separating out "A" and "B"
                    variants for our proof of concept partner.</caption>
                <thead>
                    <tr>
                        <th scope="col" rowspan="2">Week</th>
                        <th scope="col" colspan="3">Survey Type</th>
                    </tr>
                    <tr>
                        <th scope="col">Non-Partner Data<br><span class="lt-head">response rate for all other customers</span></th>
                        <th scope="col">Group "A" Variant<br><span class="lt-head">partner "A" variant response data</span></th>
                        <th scope="col">Group "B" Variant<br><span class="lt-head">partner "B" variant response data</span></th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <th scope="row">11/19 - 11/25/2018</th>
                        <td class="info">96.96%</td>
                        <td class="info">97.37%</td>
                        <td class="info">86.96%</td>
                    </tr>
                    <tr>
                        <th scope="row">11/26 - 12/2/2018</th>
                        <td class="info">96.52%</td>
                        <td class="info">100.00%</td>
                        <td class="info">83.33%</td>
                    </tr>
                    <tr>
                        <th scope="row">12/3 - 12/9/2018</th>
                        <td class="info">96.72%</td>
                        <td class="info">100.00%</td>
                        <td class="info">87.50%</td>
                    </tr>
                    <tr>
                        <th scope="row">12/10 - 12/16/2018</th>
                        <td class="info">96.48%</td>
                        <td class="info">100.00%</td>
                        <td class="info">92.86%</td>
                    </tr>
                    <tr>
                        <th scope="row">12/17 - 12/23/2018</th>
                        <td class="info">96.59%</td>
                        <td class="info">100.00%</td>
                        <td class="info">86.36%</td>
                    </tr>
                    <tr>
                        <th scope="row">12/24 - 12/30/2018</th>
                        <td class="info">96.56%</td>
                        <td class="info">95.56%</td>
                        <td class="info">92.86%</td>
                    </tr>
                </tbody>
            </table>

            <h4>Numbers Review</h4>
            <ul>
                <li>Overall, there was a <span class="h3">10.51% difference</span> between the completion rates of the
                    "A" group and the "B" group.</li>
                <li>On average, there was a <span class="h3">2.22% increase</span> in response rate for the "B" group.</li>
            </ul>
            <p>This indicates to me that the addition of the second question <span class="h3">and not any variation</span>
                caused the decrease in completion rate of the 2-question pulse.</p>

            <h2>lessons learned</h2>
            <p>I didn't have easy access to the generative research - or its insights - that was completed before I
                joined the team - and I expect that there might have been some useful information contained in it.</p>
            <p>Something that I struggled with at the beginning of the project was figuring out how my management team
                wanted to see information for the ongoing proof of concept work. To start somewhere and not get caught
                up in analysis paralysis, I focused on presenting information to our customers first so that they
                understood the value of new survey types and used that information when presenting the proof of concept
                results to my management team.</p>
            <p>Once the proof of concept was complete, I spent more time evaluating the data and presented a wrap-up
                report with that information to my management team with my recommendations.</p>

            <h2>next steps</h2>
            <p>While I had some ideas developed during this process, I knew it was important to validate those with
                customers before we started development on the features.</p>
            <p>Unfortunately, I wasn't able to test with customers before we started development on the new survey
                platform as a whole, but I was able to validate the <a href="ia-survey.html">information architecture</a>
                and work flow before we started development of the new managed survey programs.</p>
            <p>I recommended to my management team that at no point in the future we add a second question to the pulse
                using the template that we used for this A/B test. If at some point in the future, the request was
                received again from a significant number of customers, we could re-evaluate&nbsp;/ repeat the A/B test
                using the preferred variant that included signifiers indicating a multiple-step survey.</p>
        </div>

        <div class="footer">&copy; 2018 - 2019 <a href="contact.html">Malena K Hansen</a>. All rights reserved.
            Hand-coded with <i class="far fa-heart"></i> in Cleveland, OH.</div>
    </div>
</body>

</html>